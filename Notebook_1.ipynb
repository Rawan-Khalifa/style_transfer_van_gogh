{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4HKiNYOuu0D"
   },
   "source": [
    "# **2 Fetching and Downloading Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfAxLLqzWETV"
   },
   "source": [
    "## **2.1 Fetching my Google Photos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "UY8fNz9uY4Zd",
    "outputId": "b6860a44-16a5-41c3-e4f9-cc341a5879dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.151.0)\n",
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.154.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
      "Collecting google-auth\n",
      "  Using cached google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.19.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Using cached google_api_python_client-2.154.0-py2.py3-none-any.whl (12.6 MB)\n",
      "Using cached google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "Installing collected packages: google-auth, google-api-python-client\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.27.0\n",
      "    Uninstalling google-auth-2.27.0:\n",
      "      Successfully uninstalled google-auth-2.27.0\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.151.0\n",
      "    Uninstalling google-api-python-client-2.151.0:\n",
      "      Successfully uninstalled google-api-python-client-2.151.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.36.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-python-client-2.154.0 google-auth-2.36.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "28c46afc5c4342a2b5277b449a94625c",
       "pip_warning": {
        "packages": [
         "google",
         "googleapiclient"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client google-auth google-auth-oauthlib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zmc7fbUQcG2u"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u60wQa6dgkCI"
   },
   "outputs": [],
   "source": [
    "# Step 1: Authentication\n",
    "\n",
    "# Define the scope for the Google Photos API\n",
    "SCOPES = ['https://www.googleapis.com/auth/photoslibrary.readonly']\n",
    "\n",
    "# Authenticate and obtain credentials\n",
    "def authenticate_google_photos():\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    if not creds or not creds.valid:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "        creds = flow.run_local_server(port=8080)\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return creds\n",
    "\n",
    "creds = authenticate_google_photos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9sYLftO29zk"
   },
   "outputs": [],
   "source": [
    "# Step 2: Download Images\n",
    "\n",
    "def download_images_with_pagination(creds, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {creds.token}'}\n",
    "    url = 'https://photoslibrary.googleapis.com/v1/mediaItems?pageSize=10'\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('mediaItems', [])\n",
    "            print(f\"Fetched {len(items)} photos. Starting download...\")\n",
    "\n",
    "            for item in items:\n",
    "                image_url = item['baseUrl']\n",
    "                filename = item['filename']\n",
    "                try:\n",
    "                    response = requests.get(image_url)\n",
    "                    if response.status_code == 200:\n",
    "                        file_path = os.path.join(output_folder, filename)\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                        print(f\"Downloaded: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to download {filename}: {response.status_code}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {filename}: {e}\")\n",
    "\n",
    "            # Get the next page URL\n",
    "            url = data.get('nextPageToken', None)\n",
    "            if url:\n",
    "                url = f'https://photoslibrary.googleapis.com/v1/mediaItems?pageToken={url}&pageSize=10'\n",
    "        else:\n",
    "            print(f\"Error fetching photos: {response.status_code} - {response.text}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5Y_GqjC2_Zp"
   },
   "outputs": [],
   "source": [
    "# Step 3: Saving under \"my_photos\" folder in Drive\n",
    "download_images_with_pagination(creds, output_folder='/content/drive/MyDrive/Pipeline_2/my_photos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipwvbRkjZZEz"
   },
   "source": [
    "## **2.2 Fetching Van Gogh's Paintings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etsA0wlYZcN_"
   },
   "outputs": [],
   "source": [
    "# Step 1: Fetch Metadata\n",
    "def fetch_paintings(artist_name, api_key):\n",
    "    \"\"\"\n",
    "    Fetch painting metadata from the WikiArt API.\n",
    "    Args:\n",
    "        artist_name (str): URL-friendly name of the artist.\n",
    "        api_key (str): API key for WikiArt.\n",
    "    Returns:\n",
    "        list: List of painting metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE_URL}?artistUrl={artist_name}&json=2&key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Step 2: Download all images\n",
    "def download_all_paintings(paintings, output_folder):\n",
    "    \"\"\"\n",
    "    Download all paintings from the metadata list without limiting the count.\n",
    "    Args:\n",
    "        paintings (list): List of painting metadata.\n",
    "        output_folder (str): Folder to save the downloaded images.\n",
    "    \"\"\"\n",
    "    downloaded_count = 0\n",
    "    for painting in paintings:\n",
    "        title = painting[\"title\"].replace(\" \", \"_\").replace(\"/\", \"_\")  # Sanitize file name\n",
    "        image_url = painting[\"image\"]\n",
    "        file_path = os.path.join(output_folder, f\"{title}.jpg\")\n",
    "\n",
    "        # Skip if the image already exists\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Image already exists: {title}\")\n",
    "            continue\n",
    "\n",
    "        # Download and save the image\n",
    "        try:\n",
    "            response = requests.get(image_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                downloaded_count += 1\n",
    "                print(f\"Downloaded: {title} (Total downloaded: {downloaded_count})\")\n",
    "            else:\n",
    "                print(f\"Failed to download {title}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {title}: {e}\")\n",
    "\n",
    "    print(f\"Total images downloaded: {downloaded_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vsuVOh8Ypbs"
   },
   "outputs": [],
   "source": [
    "# WikiArt API Key\n",
    "API_KEY = \"23ef430d244c4ed6\"\n",
    "\n",
    "# API endpoint and artist's name\n",
    "API_BASE_URL = \"https://www.wikiart.org/en/App/Painting/PaintingsByArtist\"\n",
    "ARTIST_NAME = \"vincent-van-gogh\"  # URL-friendly name of Van Gogh\n",
    "output_folder = \"/content/drive/MyDrive/Pipeline_2/van_gogh_paintings\"\n",
    "\n",
    "# Create a folder for downloaded images\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Fetch painting data\n",
    "    paintings_data = fetch_paintings(ARTIST_NAME, API_KEY)\n",
    "    if paintings_data:\n",
    "        download_all_paintings(paintings_data, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWo3HASDSrej",
    "outputId": "6c18702e-4152-45aa-a24f-8eaf17008258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Google Photos Images is: 5108\n",
      "The number of Van Gogh Images is: 1725\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of images in each Domain\n",
    "def count_images_in_directory(directory, extensions=(\"jpg\", \"png\", \"jpeg\")):\n",
    "    count = 0\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.lower().endswith(extensions):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "van_path = \"/content/drive/MyDrive/Pipeline_2/van_gogh_paintings\"\n",
    "img_path = \"/content/drive/MyDrive/Pipeline_2/my_photos\"\n",
    "van_count = count_images_in_directory(van_path)\n",
    "img_count = count_images_in_directory(img_path)\n",
    "print(f\"The number of Google Photos Images is: {img_count}\")\n",
    "print(f\"The number of Van Gogh Images is: {van_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLbFUbSrugED"
   },
   "source": [
    "# **3 Data PreProcessing and Exploratory Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ov33GzB6DAD"
   },
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRYNAi2K9ER2",
    "outputId": "450dc8b7-6862-4ac3-cfcf-ec5cb48aa35b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.7)\n",
      "Requirement already satisfied: dominate in /usr/local/lib/python3.10/dist-packages (2.9.1)\n",
      "Requirement already satisfied: visdom in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom) (2.32.3)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom) (6.3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom) (1.16.0)\n",
      "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.10/dist-packages (from visdom) (1.33)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom) (1.8.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from visdom) (3.4.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom) (11.0.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch->visdom) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn dominate visdom torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSABYM8F8-Pl"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import shutil\n",
    "import umap.umap_ as umap\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEeOiLO5wHcC"
   },
   "source": [
    "## **3.1 Exploratory Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_sAe8-avu9P"
   },
   "source": [
    "### **3.1.1 Image Modes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JM2wBXhJg2yx"
   },
   "outputs": [],
   "source": [
    "# Check Image Modes\n",
    "def analyze_image_modes(image_dir):\n",
    "    modes = []\n",
    "    for file_name in os.listdir(image_dir):\n",
    "        if file_name.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            image_path = os.path.join(image_dir, file_name)\n",
    "            try:\n",
    "                image = Image.open(image_path)\n",
    "                modes.append(image.mode)  # Get the mode (e.g., 'RGB', 'RGBA', 'L')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "    return Counter(modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6meGRwxQ0iF",
    "outputId": "d3617bf9-d638-4c68-cc12-d55ce07b0dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Van Gogh Image Modes Count:\n",
      "RGB: 1610\n",
      "RGBA: 115\n",
      "Google Photos Image Modes Count:\n",
      "RGB: 5107\n",
      "RGBA: 1\n"
     ]
    }
   ],
   "source": [
    "van_mode_counts = analyze_image_modes(van_path)\n",
    "img_mode_counts = analyze_image_modes(img_path)\n",
    "\n",
    "print(\"Van Gogh Image Modes Count:\")\n",
    "for mode, count in van_mode_counts.items():\n",
    "    print(f\"{mode}: {count}\")\n",
    "\n",
    "print(\"Google Photos Image Modes Count:\")\n",
    "for mode, count in img_mode_counts.items():\n",
    "    print(f\"{mode}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErS8wkxdv3Qx"
   },
   "source": [
    "### **3.1.2 Image Size Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKWkV3jndIRY"
   },
   "outputs": [],
   "source": [
    "# Distribution of image sizes\n",
    "def analyze_image_sizes(image_dir):\n",
    "    widths, heights = [], []\n",
    "    for file_name in os.listdir(image_dir):\n",
    "        if file_name.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            image_path = os.path.join(image_dir, file_name)\n",
    "            image = Image.open(image_path)\n",
    "            widths.append(image.width)\n",
    "            heights.append(image.height)\n",
    "    return widths, heights\n",
    "\n",
    "# Google Photos\n",
    "widths, heights = analyze_image_sizes(img_path)\n",
    "plt.scatter(widths, heights, alpha=0.5)\n",
    "plt.title(\"Google Photos Image Size Distribution\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Height\")\n",
    "plt.show()\n",
    "\n",
    "# Van Gogh\n",
    "widths2, heights2 = analyze_image_sizes(van_path)\n",
    "plt.scatter(widths2, heights2, alpha=0.5)\n",
    "plt.title(\"Van Gogh Image Size Distribution\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Height\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUP7b2haxY2O"
   },
   "source": [
    "## **3.2 Preprocessing Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R9mWV5Wygr8"
   },
   "outputs": [],
   "source": [
    "# Global Save Path for Google Drive\n",
    "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/Pipeline_2/\"\n",
    "\n",
    "# Ensure the save path exists\n",
    "os.makedirs(DRIVE_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Defining the IMages Paths\n",
    "van_path = \"/content/drive/MyDrive/Pipeline_2/van_gogh_paintings\"\n",
    "img_path = \"/content/drive/MyDrive/Pipeline_2/my_photos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OMsk2Vh11As"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_images(image_dir, resize_dim=(512, 512), save_path=\"preprocessed_images.pt\"):\n",
    "    \"\"\"\n",
    "    Preprocess images by resizing, normalizing, and converting to tensors.\n",
    "    Args:\n",
    "        image_dir (str): Path to the directory containing images.\n",
    "        resize_dim (tuple): Dimensions to resize the images (default: 256x256).\n",
    "        save_path (str): Path to save the preprocessed images and paths (relative to DRIVE_SAVE_PATH).\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensors.\n",
    "        list: List of valid image paths.\n",
    "    \"\"\"\n",
    "    full_save_path = os.path.join(DRIVE_SAVE_PATH, save_path)\n",
    "    if os.path.exists(full_save_path):  # Load if preprocessed data already exists\n",
    "        print(f\"Loading preprocessed images from {full_save_path}...\")\n",
    "        images, image_paths = torch.load(full_save_path)\n",
    "        return images, image_paths\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(resize_dim), # Resize to 512 x 512\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\")\n",
    "    images, image_paths = [], []\n",
    "\n",
    "    for fname in os.listdir(image_dir):\n",
    "        if fname.lower().endswith(valid_extensions):\n",
    "            try:\n",
    "                img_path = os.path.join(image_dir, fname)\n",
    "                img = Image.open(img_path).convert(\"RGB\") # Convert images to RGB\n",
    "                images.append(transform(img))\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {fname}: {e}\")\n",
    "\n",
    "    images = torch.stack(images)  # Combine into a tensor\n",
    "    torch.save((images, image_paths), full_save_path)  # Save for reuse\n",
    "    print(f\"Preprocessed {len(images)} images and saved to {full_save_path}.\")\n",
    "    return images, image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE7KVOyxzYRR"
   },
   "source": [
    "## **3.3 Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiuBAVpN25TT"
   },
   "source": [
    "### 3.3.1 **Feature Extraction with ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_KcKaWV2_92",
    "outputId": "1e450980-5e5d-450e-e950-94a34c1d4a81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet-50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet = resnet50(pretrained=True).to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# Remove the final classification layer\n",
    "feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9S7K0A7CDz6"
   },
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "def extract_features_in_batches(images, model, batch_size=16, save_path=\"extracted_features.npy\"):\n",
    "    \"\"\"\n",
    "    Extract features from images using a pretrained ResNet model in batches.\n",
    "    Args:\n",
    "        images (torch.Tensor): Batch of preprocessed images (N x 3 x H x W).\n",
    "        model (nn.Module): Feature extractor (ResNet without the FC layer).\n",
    "        batch_size (int): Size of each mini-batch.\n",
    "        save_path (str): Path to save the extracted features (relative to DRIVE_SAVE_PATH).\n",
    "    Returns:\n",
    "        np.ndarray: Extracted features (N x 2048).\n",
    "    \"\"\"\n",
    "    full_save_path = os.path.join(DRIVE_SAVE_PATH, save_path)\n",
    "    if os.path.exists(full_save_path):  # Load if features already exist\n",
    "        print(f\"Loading extracted features from {full_save_path}...\")\n",
    "        return np.load(full_save_path)\n",
    "\n",
    "    model = model.to(device)\n",
    "    features = []\n",
    "\n",
    "    # Process images in mini-batches\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i + batch_size].to(device)  # Slice batch\n",
    "            batch_features = model(batch).view(batch.size(0), -1)  # Flatten features\n",
    "            features.append(batch_features.cpu().numpy())  # Move to CPU and store\n",
    "\n",
    "            # Progress log\n",
    "            print(f\"Processed batch {i // batch_size + 1}/{(len(images) + batch_size - 1) // batch_size}\", end=\"\\r\")\n",
    "\n",
    "    features = np.vstack(features)  # Combine all batch features\n",
    "    np.save(full_save_path, features)  # Save for reuse\n",
    "    print(f\"\\nExtracted features saved to {full_save_path}.\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8rLoVSOz12k"
   },
   "source": [
    "### **3.3.2 t-SNE for Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMMQ5msnF2IA"
   },
   "outputs": [],
   "source": [
    "# Function to create t-SNE visualization with images\n",
    "def visualize_tsne_with_images(features, image_paths, sample_size, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualize t-SNE embeddings with image thumbnails.\n",
    "    Args:\n",
    "        features (np.ndarray): High-dimensional features (N x D).\n",
    "        image_paths (list): List of corresponding image paths (N elements).\n",
    "        sample_size (int): Number of images to sample for visualization.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Sample a subset of images and features\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.choice(len(features), size=sample_size, replace=False)\n",
    "    sampled_features = features[indices]\n",
    "    sampled_paths = [image_paths[i] for i in indices]\n",
    "\n",
    "    # Apply t-SNE to the sampled features\n",
    "    tsne = TSNE(n_components=2, random_state=random_state, perplexity=30, n_iter=1000)\n",
    "    tsne_embeddings = tsne.fit_transform(sampled_features)\n",
    "\n",
    "    # Normalize t-SNE embeddings for better placement\n",
    "    x_min, x_max = tsne_embeddings.min(0), tsne_embeddings.max(0)\n",
    "    tsne_embeddings = (tsne_embeddings - x_min) / (x_max - x_min)\n",
    "\n",
    "    # Visualize with image thumbnails\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    for i, (x, y) in enumerate(tsne_embeddings):\n",
    "        # Load and resize the image\n",
    "        img = Image.open(sampled_paths[i]).convert(\"RGB\").resize((30, 30))\n",
    "        ax.imshow(img, extent=(x - 0.015, x + 0.015, y - 0.015, y + 0.015), aspect=\"auto\")\n",
    "\n",
    "    ax.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], alpha=0.5, s=10, c='gray')  # Scatter plot for reference\n",
    "    ax.set_title(\"t-SNE Visualization with Image Thumbnails\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYiEhCmV1Nr4"
   },
   "outputs": [],
   "source": [
    "# Ensure features and image paths are extracted\n",
    "images, image_paths = preprocess_images(img_path, save_path=\"preprocessed.pt\")\n",
    "features = extract_features_in_batches(images, feature_extractor, save_path=\"features.npy\")\n",
    "\n",
    "# Testing it out on the raw ResNet features for My Photos\n",
    "visualize_tsne_with_images(features=features, image_paths=image_paths, sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZcyrSk51vJT"
   },
   "outputs": [],
   "source": [
    "# Testing it out on the raw ResNet features for Van Gogh's Paintings\n",
    "images, image_paths = preprocess_images(van_path, save_path=\"van_preprocessed.pt\")\n",
    "features = extract_features_in_batches(images, feature_extractor, save_path=\"van_features.npy\")\n",
    "\n",
    "# Visualize Van Gogh's Paintings\n",
    "visualize_tsne_with_images(features=features, image_paths=image_paths, sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLbQjiUA4iya"
   },
   "source": [
    "## **3.4 Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnVWsgObC1_5"
   },
   "outputs": [],
   "source": [
    "# Dimensionality Reduction (PCA + UMAP)\n",
    "def reduce_dimensions(features, save_path=\"reduced_features.npy\", n_pca_components=300, n_umap_components=2, n_neighbors=15, min_dist=0.1):\n",
    "    \"\"\"\n",
    "    Reduce dimensions using PCA followed by UMAP.\n",
    "    Args:\n",
    "        features (np.ndarray): High-dimensional feature vectors.\n",
    "        save_path (str): Path to save the reduced features (relative to DRIVE_SAVE_PATH).\n",
    "    Returns:\n",
    "        np.ndarray: 2D UMAP-reduced features.\n",
    "    \"\"\"\n",
    "    full_save_path = os.path.join(DRIVE_SAVE_PATH, save_path)\n",
    "    if os.path.exists(full_save_path):  # Load if reduced features already exist\n",
    "        print(f\"Loading reduced features from {full_save_path}...\")\n",
    "        return np.load(full_save_path)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_pca_components, random_state=42)\n",
    "    pca_features = pca.fit_transform(features)\n",
    "    print(f\"PCA retained {np.sum(pca.explained_variance_ratio_) * 100:.2f}% variance.\")\n",
    "    # UMAP\n",
    "    umap_reducer = umap.UMAP(n_components=n_umap_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)\n",
    "    reduced_features = umap_reducer.fit_transform(pca_features)\n",
    "    np.save(full_save_path, reduced_features)  # Save for reuse\n",
    "    print(f\"Reduced features saved to {full_save_path}.\")\n",
    "    return reduced_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS_cp8Yt6nNp"
   },
   "source": [
    "### **3.4.4 Visualize UMAP embeddings with image thumbnails**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myKBN5TdhAWq"
   },
   "outputs": [],
   "source": [
    "# Function to create UMAP visualization with image thumbnails\n",
    "def visualize_umap_with_images(umap_embeddings, image_paths, sample_size=200, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualize UMAP embeddings with image thumbnails.\n",
    "    Args:\n",
    "        umap_embeddings (np.ndarray): 2D UMAP embeddings (N x 2).\n",
    "        image_paths (list): List of corresponding image paths (N elements).\n",
    "        sample_size (int): Number of images to sample for visualization.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Sample a subset of images and embeddings\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.choice(len(umap_embeddings), size=sample_size, replace=False)\n",
    "    sampled_embeddings = umap_embeddings[indices]\n",
    "    sampled_paths = [image_paths[i] for i in indices]\n",
    "\n",
    "    # Normalize UMAP embeddings for better placement\n",
    "    x_min, x_max = sampled_embeddings.min(0), sampled_embeddings.max(0)\n",
    "    normalized_embeddings = (sampled_embeddings - x_min) / (x_max - x_min)\n",
    "\n",
    "    # Visualize with image thumbnails\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    for i, (x, y) in enumerate(normalized_embeddings):\n",
    "        # Load and resize the image\n",
    "        img = Image.open(sampled_paths[i]).convert(\"RGB\").resize((30, 30))\n",
    "        ax.imshow(img, extent=(x - 0.015, x + 0.015, y - 0.015, y + 0.015), aspect=\"auto\")\n",
    "\n",
    "    ax.scatter(normalized_embeddings[:, 0], normalized_embeddings[:, 1], alpha=0.5, s=10, c='gray')  # Scatter plot for reference\n",
    "    ax.set_title(\"UMAP Visualization with Image Thumbnails\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga4t9-L64ZXl"
   },
   "outputs": [],
   "source": [
    "# Applying Dimentionality Reduction to My Photos\n",
    "images, image_paths = preprocess_images(img_path, save_path=\"preprocessed.pt\")\n",
    "features = extract_features_in_batches(images, feature_extractor, save_path=\"features.npy\")\n",
    "reduced_features = reduce_dimensions(features, save_path=\"reduced.npy\")\n",
    "\n",
    "visualize_umap_with_images(umap_embeddings=reduced_features, image_paths=image_paths, sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yEGt2v11G3j"
   },
   "outputs": [],
   "source": [
    "# Applying Dimentionality Reduction to Van Gogh's\n",
    "images, image_paths = preprocess_images(van_path, save_path=\"van_preprocessed.pt\")\n",
    "features = extract_features_in_batches(images, feature_extractor, save_path=\"van_features.npy\")\n",
    "reduced_features = reduce_dimensions(features, save_path=\"van_reduced.npy\")\n",
    "\n",
    "visualize_umap_with_images(umap_embeddings=reduced_features, image_paths=image_paths, sample_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz37jYJZ41mA"
   },
   "source": [
    "## **3.5 Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kec1HVCk730q"
   },
   "source": [
    "### **3.5.1 Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO787h71xcqz"
   },
   "outputs": [],
   "source": [
    "# Evaluate Clusters\n",
    "def evaluate_clustering(data, labels, algorithm_name):\n",
    "    if len(set(labels)) > 1:  # Ensure more than one cluster is formed\n",
    "        silhouette = silhouette_score(data, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(data, labels)\n",
    "        print(f\"{algorithm_name} Metrics:\")\n",
    "        print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz Index: {calinski_harabasz:.4f}\")\n",
    "        print(f\"  Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "        return silhouette, calinski_harabasz, davies_bouldin\n",
    "    else:\n",
    "        print(f\"{algorithm_name} Metrics: No valid clusters formed.\")\n",
    "        return -1, -1, -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImzF9_kM8CI0",
    "outputId": "219a2bf8-7019-49ea-82cf-a25295b8ec3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed images from /content/drive/MyDrive/Pipeline_2/preprocessed.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-d92b1f5f80be>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  images, image_paths = torch.load(full_save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extracted features from /content/drive/MyDrive/Pipeline_2/features.npy...\n",
      "Loading reduced features from /content/drive/MyDrive/Pipeline_2/reduced.npy...\n"
     ]
    }
   ],
   "source": [
    "images, image_paths = preprocess_images(img_path, save_path=\"preprocessed.pt\")\n",
    "features = extract_features_in_batches(images, feature_extractor, save_path=\"features.npy\")\n",
    "reduced_features = reduce_dimensions(features, save_path=\"reduced.npy\")\n",
    "\n",
    "data = reduced_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HdekMlNciKV",
    "outputId": "5e1094e6-ca29-4daa-b63d-0710b103291b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN Metrics:\n",
      "  Silhouette Score: 0.2605\n",
      "  Calinski-Harabasz Index: 229.7848\n",
      "  Davies-Bouldin Index: 1.3661\n",
      "K-Means Metrics:\n",
      "  Silhouette Score: 0.4670\n",
      "  Calinski-Harabasz Index: 4245.8340\n",
      "  Davies-Bouldin Index: 0.8088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.46696675, 4245.834033398844, 0.8087504754742091)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(data)\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(data)\n",
    "\n",
    "evaluate_clustering(data, dbscan_labels, \"DBSCAN\")\n",
    "evaluate_clustering(data, kmeans_labels, \"K-Means\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "_YL_cnFBkuvY",
    "outputId": "24ec5cd9-3e54-4891-89c7-1d50ba9c073c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Metrics:\n",
      "  Silhouette Score: 0.4670\n",
      "  Calinski-Harabasz Index: 4245.8340\n",
      "  Davies-Bouldin Index: 0.8088\n",
      "K-Means Metrics:\n",
      "  Silhouette Score: 0.3854\n",
      "  Calinski-Harabasz Index: 3732.3416\n",
      "  Davies-Bouldin Index: 0.9129\n",
      "K-Means Metrics:\n",
      "  Silhouette Score: 0.4036\n",
      "  Calinski-Harabasz Index: 3268.2861\n",
      "  Davies-Bouldin Index: 0.7773\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"n_clusters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          4,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"silhouette\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.46696674823760986,\n          0.38535502552986145,\n          0.40356525778770447\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"calinski_harabasz\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 488.9822905079735,\n        \"min\": 3268.28605327818,\n        \"max\": 4245.834033398844,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4245.834033398844,\n          3732.3416046180027,\n          3268.28605327818\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"davies_bouldin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07097231422756689,\n        \"min\": 0.7772637027585342,\n        \"max\": 0.9128722029669539,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8087504754742091,\n          0.9128722029669539,\n          0.7772637027585342\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "results_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5b52389f-0bb3-4505-b454-d1ae815376dd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>davies_bouldin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.466967</td>\n",
       "      <td>4245.834033</td>\n",
       "      <td>0.808750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.385355</td>\n",
       "      <td>3732.341605</td>\n",
       "      <td>0.912872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.403565</td>\n",
       "      <td>3268.286053</td>\n",
       "      <td>0.777264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b52389f-0bb3-4505-b454-d1ae815376dd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5b52389f-0bb3-4505-b454-d1ae815376dd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5b52389f-0bb3-4505-b454-d1ae815376dd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-ec25ecb0-7a50-4437-bc63-6468c9874567\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec25ecb0-7a50-4437-bc63-6468c9874567')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-ec25ecb0-7a50-4437-bc63-6468c9874567 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_7185dcd7-fd4c-4847-8976-4319c6a4324b\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_7185dcd7-fd4c-4847-8976-4319c6a4324b button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('results_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   n_clusters  silhouette  calinski_harabasz  davies_bouldin\n",
       "0           3    0.466967        4245.834033        0.808750\n",
       "1           4    0.385355        3732.341605        0.912872\n",
       "2           5    0.403565        3268.286053        0.777264"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through different numbers of clusters\n",
    "results = []\n",
    "for n_clusters in [3, 4, 5]:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(data)\n",
    "    silhouette, calinski_harabasz, davies_bouldin = evaluate_clustering(data, kmeans_labels, \"K-Means\")\n",
    "    results.append({\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"silhouette\": silhouette,\n",
    "        \"calinski_harabasz\": calinski_harabasz,\n",
    "        \"davies_bouldin\": davies_bouldin\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7sTy1M-8e2T"
   },
   "source": [
    "### **3.5.3 Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XF4yi1-k-lI0"
   },
   "outputs": [],
   "source": [
    "# Updated to integrate the reduced features directly and fit within the pipeline\n",
    "\n",
    "def visualize_clusters(reduced_features, labels, title=\"Clustering Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualize clusters in the reduced feature space.\n",
    "    Args:\n",
    "        reduced_features (np.ndarray): 2D reduced feature space (e.g., UMAP output).\n",
    "        labels (np.ndarray): Cluster labels for the data points.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.tab10(i / float(len(unique_labels) - 1)) for i in range(len(unique_labels))]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:  # Noise points\n",
    "            col = [0, 0, 0, 1]  # Black for noise\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "        xy = reduced_features[class_member_mask]\n",
    "        plt.scatter(xy[:, 0], xy[:, 1], s=50, c=[col], label=f\"Cluster {k}\" if k != -1 else \"Noise\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02RmLvIN8h-B"
   },
   "outputs": [],
   "source": [
    "visualize_clusters(data, dbscan_labels, \"DBSCAN Clustering\")\n",
    "visualize_clusters(data, kmeans_labels, \"K-Means Clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIDlobQK9bRr"
   },
   "source": [
    "### **3.5.2 Grid Search for DBSCAN Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM6qZ4bA8g0h",
    "outputId": "894a0876-3fcb-4044-a2d8-20ec1adc6b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "eps                    0.100000\n",
      "min_samples            5.000000\n",
      "silhouette             0.260460\n",
      "calinski_harabasz    229.784779\n",
      "davies_bouldin         1.366059\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'eps': np.linspace(0.1, 2.0, 20),  # 20 values for eps between 0.1 and 2.0\n",
    "    'min_samples': range(2, 10)  # Values for min_samples between 2 and 9\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Best score tracking\n",
    "best_score = -1\n",
    "best_params = {}\n",
    "\n",
    "# Function to evaluate clustering\n",
    "def evaluate_clustering(data, labels):\n",
    "    if len(set(labels)) > 1:  # Ensure more than one cluster is formed\n",
    "        silhouette = silhouette_score(data, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(data, labels)\n",
    "        return silhouette, calinski_harabasz, davies_bouldin\n",
    "    else:\n",
    "        return -1, -1, -1  # Invalid clustering\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "\n",
    "for params in grid:\n",
    "    dbscan = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
    "    labels = dbscan.fit_predict(data)\n",
    "    silhouette, calinski_harabasz, davies_bouldin = evaluate_clustering(data, labels)\n",
    "\n",
    "    results.append({\n",
    "        'eps': params['eps'],\n",
    "        'min_samples': params['min_samples'],\n",
    "        'silhouette': silhouette,\n",
    "        'calinski_harabasz': calinski_harabasz,\n",
    "        'davies_bouldin': davies_bouldin\n",
    "    })\n",
    "\n",
    "    if silhouette > best_score:\n",
    "        best_score = silhouette\n",
    "        best_params = params\n",
    "\n",
    "# Display the best parameters\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "best_result = results_df.loc[results_df['silhouette'].idxmax()]\n",
    "print(\"Best Parameters:\")\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "SxERrwM4u7Uy",
    "outputId": "d1b29b0a-6c9a-41f3-a05f-ed28881b9756"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 160,\n  \"fields\": [\n    {\n      \"column\": \"eps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5784385835752603,\n        \"min\": 0.1,\n        \"max\": 2.0,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.1,\n          0.6,\n          0.7999999999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_samples\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2,\n        \"max\": 9,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          4,\n          7,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"silhouette\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 45,\n        \"samples\": [\n          -0.3483641445636749,\n          -0.15322697162628174,\n          -0.1661502867937088\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"calinski_harabasz\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 129.16097842778436,\n        \"min\": 96.91017313340254,\n        \"max\": 981.7573761471332,\n        \"num_unique_values\": 45,\n        \"samples\": [\n          134.686243634546,\n          334.7292145015012,\n          337.99120185595064\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"davies_bouldin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.37813433222428755,\n        \"min\": 0.35065553004765304,\n        \"max\": 2.7897059008516845,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          0.7159299754453217,\n          1.1186186915110115,\n          1.1309086082962594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "results_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-fa515f88-2bc3-4a4d-86bb-eb911fbed886\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eps</th>\n",
       "      <th>min_samples</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>davies_bouldin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.260460</td>\n",
       "      <td>229.784779</td>\n",
       "      <td>1.366059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.257073</td>\n",
       "      <td>307.200105</td>\n",
       "      <td>1.395067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.252895</td>\n",
       "      <td>452.349077</td>\n",
       "      <td>1.491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.244771</td>\n",
       "      <td>544.411119</td>\n",
       "      <td>1.566227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.244717</td>\n",
       "      <td>217.226379</td>\n",
       "      <td>0.350656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.364013</td>\n",
       "      <td>119.118189</td>\n",
       "      <td>1.041764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.4</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.364013</td>\n",
       "      <td>119.118189</td>\n",
       "      <td>1.041764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.364013</td>\n",
       "      <td>119.118189</td>\n",
       "      <td>1.041764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.365997</td>\n",
       "      <td>115.676516</td>\n",
       "      <td>0.962927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.379659</td>\n",
       "      <td>106.082222</td>\n",
       "      <td>0.704286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 5 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa515f88-2bc3-4a4d-86bb-eb911fbed886')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-fa515f88-2bc3-4a4d-86bb-eb911fbed886 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-fa515f88-2bc3-4a4d-86bb-eb911fbed886');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-673a4e80-60dd-4804-ba7d-3f32e85ace73\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-673a4e80-60dd-4804-ba7d-3f32e85ace73')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-673a4e80-60dd-4804-ba7d-3f32e85ace73 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_70d8f771-48b4-49bb-b192-e500daf7a22b\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_70d8f771-48b4-49bb-b192-e500daf7a22b button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('results_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     eps  min_samples  silhouette  calinski_harabasz  davies_bouldin\n",
       "0    0.1            5    0.260460         229.784779        1.366059\n",
       "1    0.1            4    0.257073         307.200105        1.395067\n",
       "2    0.1            3    0.252895         452.349077        1.491732\n",
       "3    0.1            2    0.244771         544.411119        1.566227\n",
       "4    1.9            5    0.244717         217.226379        0.350656\n",
       "..   ...          ...         ...                ...             ...\n",
       "155  0.4            5   -0.364013         119.118189        1.041764\n",
       "156  0.4            6   -0.364013         119.118189        1.041764\n",
       "157  0.4            4   -0.364013         119.118189        1.041764\n",
       "158  0.4            3   -0.365997         115.676516        0.962927\n",
       "159  0.4            2   -0.379659         106.082222        0.704286\n",
       "\n",
       "[160 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column to rank by Silhouette Score\n",
    "results_df = results_df.sort_values(by=\"silhouette\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Print the best results df\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPPTFJre_5vO"
   },
   "source": [
    "### **3.5.4 Clustering and Saving Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnAle5ho9yHX"
   },
   "outputs": [],
   "source": [
    "# Step 4: Clustering\n",
    "def cluster_features(features, save_path=\"cluster_labels.npy\", method=\"dbscan\", n_clusters=5, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Cluster features using DBSCAN or K-Means.\n",
    "    Args:\n",
    "        features (np.ndarray): Reduced features for clustering.\n",
    "        save_path (str): Path to save the cluster labels (relative to DRIVE_SAVE_PATH).\n",
    "    Returns:\n",
    "        np.ndarray: Cluster labels.\n",
    "    \"\"\"\n",
    "    full_save_path = os.path.join(DRIVE_SAVE_PATH, save_path)\n",
    "    if os.path.exists(full_save_path):  # Load if cluster labels already exist\n",
    "        print(f\"Loading cluster labels from {full_save_path}...\")\n",
    "        return np.load(full_save_path)\n",
    "\n",
    "    if method == \"dbscan\":\n",
    "        clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    elif method == \"kmeans\":\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported clustering method. Use 'dbscan' or 'kmeans'.\")\n",
    "\n",
    "    labels = clusterer.fit_predict(features)\n",
    "    np.save(full_save_path, labels)  # Save for reuse\n",
    "    print(f\"Cluster labels saved to {full_save_path}.\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOzR9rNB7mo-"
   },
   "outputs": [],
   "source": [
    "# Step 5: Save Clustered Images\n",
    "def save_clustered_images(image_paths, labels, output_dir=\"clustered_images\"):\n",
    "    \"\"\"\n",
    "    Save clustered images into separate folders based on cluster labels.\n",
    "    \"\"\"\n",
    "    full_output_dir = os.path.join(DRIVE_SAVE_PATH, output_dir)\n",
    "    os.makedirs(full_output_dir, exist_ok=True)\n",
    "    for label in set(labels):\n",
    "        cluster_dir = os.path.join(full_output_dir, f\"cluster_{label}\")\n",
    "        os.makedirs(cluster_dir, exist_ok=True)\n",
    "    for img_path, label in zip(image_paths, labels):\n",
    "        if label == -1:  # Skip noise points (for DBSCAN)\n",
    "            continue\n",
    "        shutil.copy(img_path, os.path.join(cluster_dir, os.path.basename(img_path)))\n",
    "    print(f\"Clustered images saved to '{full_output_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7QjF4mVAT41"
   },
   "source": [
    "## **Run the Full PreProcessing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZLtQYmuA2rM"
   },
   "outputs": [],
   "source": [
    "# Full Pipeline Function\n",
    "def full_pipeline(image_dir, output_prefix=\"pipeline\", clustering_method=\"dbscan\"):\n",
    "    \"\"\"\n",
    "    Full pipeline for preprocessing, feature extraction, dimensionality reduction, and clustering.\n",
    "    Args:\n",
    "        image_dir (str): Path to the directory containing images.\n",
    "        output_prefix (str): Prefix for saved files (relative to DRIVE_SAVE_PATH).\n",
    "        clustering_method (str): Clustering method ('dbscan' or 'kmeans').\n",
    "    \"\"\"\n",
    "    images, image_paths = preprocess_images(image_dir, save_path=f\"{output_prefix}_preprocessed.pt\")\n",
    "    features = extract_features(images, feature_extractor, save_path=f\"{output_prefix}_features.npy\")\n",
    "    reduced_features = reduce_dimensions(features, save_path=f\"{output_prefix}_reduced.npy\")\n",
    "    labels = cluster_features(reduced_features, save_path=f\"{output_prefix}_labels.npy\", method=clustering_method)\n",
    "    save_clustered_images(image_paths, labels, output_dir=f\"{output_prefix}_clustered\")\n",
    "    return labels\n",
    "\n",
    "# Apply Pipeline\n",
    "full_pipeline(img_path, output_prefix=\"your_images\", clustering_method=\"dbscan\")\n",
    "full_pipeline(van_path, output_prefix=\"vangogh_images\", clustering_method=\"dbscan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s411eoJuHwSy"
   },
   "source": [
    "---\n",
    "# **4 CycleGAN**\n",
    "\n",
    "\n",
    "\n",
    "## **4.2 Model Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ho2ZceVUIBPk"
   },
   "source": [
    "### **Paper-Based Discriminator Archeticture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NY20zCvaIK5D"
   },
   "outputs": [],
   "source": [
    "# Create a block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=True, padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        # Initial layer\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                features[0],\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # Building the layers dynamically\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(Block(in_channels, feature, stride=1 if feature == features[-1] else 2))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Final layer\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")\n",
    "        )\n",
    "\n",
    "        # Convert the layers list to a Sequential module\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.model(x)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kp9vbDeEI1iK",
    "outputId": "c5ee8135-12a6-496e-a2b3-718bb648eed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           3,136\n",
      "         LeakyReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3        [-1, 128, 128, 128]         131,200\n",
      "    InstanceNorm2d-4        [-1, 128, 128, 128]               0\n",
      "         LeakyReLU-5        [-1, 128, 128, 128]               0\n",
      "             Block-6        [-1, 128, 128, 128]               0\n",
      "            Conv2d-7          [-1, 256, 64, 64]         524,544\n",
      "    InstanceNorm2d-8          [-1, 256, 64, 64]               0\n",
      "         LeakyReLU-9          [-1, 256, 64, 64]               0\n",
      "            Block-10          [-1, 256, 64, 64]               0\n",
      "           Conv2d-11          [-1, 512, 63, 63]       2,097,664\n",
      "   InstanceNorm2d-12          [-1, 512, 63, 63]               0\n",
      "        LeakyReLU-13          [-1, 512, 63, 63]               0\n",
      "            Block-14          [-1, 512, 63, 63]               0\n",
      "           Conv2d-15            [-1, 1, 62, 62]           8,193\n",
      "================================================================\n",
      "Total params: 2,764,737\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 222.04\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 235.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Provide summary of the model archeticture\n",
    "model = Discriminator().cuda()\n",
    "summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-p_EBDKIY6j"
   },
   "source": [
    "### **Paper-Based Generator Archeticture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6z1X44LwIddy"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs) # the keyward arguments are the kernel size, stride and padding\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=64, num_residuals=9):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
    "                ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n",
    "            ]\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n",
    "        )\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                ConvBlock(num_features*2, num_features*1, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ]\n",
    "        )\n",
    "        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x = layer(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x = layer(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWRMiYj0JEQ1",
    "outputId": "ae7219d3-470f-45e0-883e-256506850ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           9,472\n",
      "              ReLU-2         [-1, 64, 512, 512]               0\n",
      "            Conv2d-3        [-1, 128, 256, 256]          73,856\n",
      "    InstanceNorm2d-4        [-1, 128, 256, 256]               0\n",
      "              ReLU-5        [-1, 128, 256, 256]               0\n",
      "         ConvBlock-6        [-1, 128, 256, 256]               0\n",
      "            Conv2d-7        [-1, 256, 128, 128]         295,168\n",
      "    InstanceNorm2d-8        [-1, 256, 128, 128]               0\n",
      "              ReLU-9        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-10        [-1, 256, 128, 128]               0\n",
      "           Conv2d-11        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-12        [-1, 256, 128, 128]               0\n",
      "             ReLU-13        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-14        [-1, 256, 128, 128]               0\n",
      "           Conv2d-15        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-16        [-1, 256, 128, 128]               0\n",
      "         Identity-17        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-18        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-19        [-1, 256, 128, 128]               0\n",
      "           Conv2d-20        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-21        [-1, 256, 128, 128]               0\n",
      "             ReLU-22        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-23        [-1, 256, 128, 128]               0\n",
      "           Conv2d-24        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-25        [-1, 256, 128, 128]               0\n",
      "         Identity-26        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-27        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-28        [-1, 256, 128, 128]               0\n",
      "           Conv2d-29        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-30        [-1, 256, 128, 128]               0\n",
      "             ReLU-31        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-32        [-1, 256, 128, 128]               0\n",
      "           Conv2d-33        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-34        [-1, 256, 128, 128]               0\n",
      "         Identity-35        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-36        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-37        [-1, 256, 128, 128]               0\n",
      "           Conv2d-38        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-39        [-1, 256, 128, 128]               0\n",
      "             ReLU-40        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-41        [-1, 256, 128, 128]               0\n",
      "           Conv2d-42        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-43        [-1, 256, 128, 128]               0\n",
      "         Identity-44        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-45        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-46        [-1, 256, 128, 128]               0\n",
      "           Conv2d-47        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-48        [-1, 256, 128, 128]               0\n",
      "             ReLU-49        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-50        [-1, 256, 128, 128]               0\n",
      "           Conv2d-51        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-52        [-1, 256, 128, 128]               0\n",
      "         Identity-53        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-54        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-55        [-1, 256, 128, 128]               0\n",
      "           Conv2d-56        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-57        [-1, 256, 128, 128]               0\n",
      "             ReLU-58        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-59        [-1, 256, 128, 128]               0\n",
      "           Conv2d-60        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-61        [-1, 256, 128, 128]               0\n",
      "         Identity-62        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-63        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-64        [-1, 256, 128, 128]               0\n",
      "           Conv2d-65        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-66        [-1, 256, 128, 128]               0\n",
      "             ReLU-67        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-68        [-1, 256, 128, 128]               0\n",
      "           Conv2d-69        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-70        [-1, 256, 128, 128]               0\n",
      "         Identity-71        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-72        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-73        [-1, 256, 128, 128]               0\n",
      "           Conv2d-74        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-75        [-1, 256, 128, 128]               0\n",
      "             ReLU-76        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-77        [-1, 256, 128, 128]               0\n",
      "           Conv2d-78        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-79        [-1, 256, 128, 128]               0\n",
      "         Identity-80        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-81        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-82        [-1, 256, 128, 128]               0\n",
      "           Conv2d-83        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-84        [-1, 256, 128, 128]               0\n",
      "             ReLU-85        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-86        [-1, 256, 128, 128]               0\n",
      "           Conv2d-87        [-1, 256, 128, 128]         590,080\n",
      "   InstanceNorm2d-88        [-1, 256, 128, 128]               0\n",
      "         Identity-89        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-90        [-1, 256, 128, 128]               0\n",
      "    ResidualBlock-91        [-1, 256, 128, 128]               0\n",
      "  ConvTranspose2d-92        [-1, 128, 256, 256]         295,040\n",
      "   InstanceNorm2d-93        [-1, 128, 256, 256]               0\n",
      "             ReLU-94        [-1, 128, 256, 256]               0\n",
      "        ConvBlock-95        [-1, 128, 256, 256]               0\n",
      "  ConvTranspose2d-96         [-1, 64, 512, 512]          73,792\n",
      "   InstanceNorm2d-97         [-1, 64, 512, 512]               0\n",
      "             ReLU-98         [-1, 64, 512, 512]               0\n",
      "        ConvBlock-99         [-1, 64, 512, 512]               0\n",
      "          Conv2d-100          [-1, 3, 512, 512]           9,411\n",
      "================================================================\n",
      "Total params: 11,378,179\n",
      "Trainable params: 11,378,179\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 4006.00\n",
      "Params size (MB): 43.40\n",
      "Estimated Total Size (MB): 4052.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Provide summary of the model archeticture\n",
    "model = Generator(img_channels=3).cuda()\n",
    "summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oha6nS4HLnVY"
   },
   "source": [
    "## **Simpler Model Archeticture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e984QOZQL0vX"
   },
   "source": [
    "### **Discriminator Archeticture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kta7vXF_L_qD"
   },
   "outputs": [],
   "source": [
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtcWF5MGL3S6"
   },
   "source": [
    "### **Generator Archeticture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHVqDf_MLwYM"
   },
   "outputs": [],
   "source": [
    "# Define Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzbtbqXtJQ1A"
   },
   "source": [
    "## **4.3.1 Loss Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Oe501CVJWZy"
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "class CycleGANLosses:\n",
    "    def __init__(self, lambda_cycle=10.0, lambda_identity=5.0):\n",
    "        self.adversarial_loss = nn.MSELoss()\n",
    "        self.cycle_loss = nn.L1Loss()\n",
    "        self.identity_loss = nn.L1Loss()\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "    def generator_loss(self, fake_output, real_images, reconstructed_images, identity_images=None):\n",
    "        gan_loss = self.adversarial_loss(fake_output, torch.ones_like(fake_output))\n",
    "        cycle_loss = self.cycle_loss(reconstructed_images, real_images) * self.lambda_cycle\n",
    "        identity_loss = self.identity_loss(identity_images, real_images) * self.lambda_identity if identity_images is not None else 0.0\n",
    "        return gan_loss + cycle_loss + identity_loss\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_loss = self.adversarial_loss(real_output, torch.ones_like(real_output))\n",
    "        fake_loss = self.adversarial_loss(fake_output, torch.zeros_like(fake_output))\n",
    "        return (real_loss + fake_loss) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQiunZL-Isvf"
   },
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfp4CahWJ8eM"
   },
   "source": [
    "## **Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fF3yvyjkJoc8"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset Loader for Unlabeled Images\n",
    "class UnpairedImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Load Dataset\n",
    "def load_data(data_dir, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Using 512x512 or 256 x 256 resolution\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean= [0.5, 0.5, 0.5], std=[0.5, 0.5,0.5])\n",
    "    ])\n",
    "\n",
    "\n",
    "    dataset = UnpairedImageDataset(data_dir, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AVRo40RKYTA"
   },
   "source": [
    "## **Defining the Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C1bRiSeKMS8"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_cycle_gan(generator_A2B, generator_B2A, discriminator_A, discriminator_B,\n",
    "                    dataloader_A, dataloader_B, optimizer_G, optimizer_D,\n",
    "                    loss_fn, epochs, device, checkpoint_dir=\"checkpoints\"):\n",
    "    # Ensure the checkpoint directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize lists to store losses\n",
    "    generator_losses = []\n",
    "    discriminator_losses = []\n",
    "\n",
    "    # Load from checkpoint if exists\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(os.path.join(checkpoint_dir, \"last_checkpoint.pth\")):\n",
    "        print(\"Loading from checkpoint...\")\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, \"last_checkpoint.pth\"))\n",
    "        generator_A2B.load_state_dict(checkpoint[\"generator_A2B\"])\n",
    "        generator_B2A.load_state_dict(checkpoint[\"generator_B2A\"])\n",
    "        discriminator_A.load_state_dict(checkpoint[\"discriminator_A\"])\n",
    "        discriminator_B.load_state_dict(checkpoint[\"discriminator_B\"])\n",
    "        optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
    "        optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
    "        generator_losses = checkpoint[\"generator_losses\"]\n",
    "        discriminator_losses = checkpoint[\"discriminator_losses\"]\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        epoch_generator_loss = 0.0\n",
    "        epoch_discriminator_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for i, (real_A, real_B) in enumerate(zip(dataloader_A, cycle(dataloader_B))):\n",
    "            num_batches += 1\n",
    "            real_A = real_A.to(device)\n",
    "            real_B = real_B.to(device)\n",
    "\n",
    "            # ------------------------\n",
    "            # Train Generators\n",
    "            # ------------------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            fake_B = generator_A2B(real_A)\n",
    "            reconstructed_A = generator_B2A(fake_B)\n",
    "            fake_A = generator_B2A(real_B)\n",
    "            reconstructed_B = generator_A2B(fake_A)\n",
    "\n",
    "            identity_A = generator_B2A(real_A)\n",
    "            identity_B = generator_A2B(real_B)\n",
    "\n",
    "            # Compute generator losses\n",
    "            loss_G_A2B = loss_fn.generator_loss(discriminator_B(fake_B), real_A, reconstructed_A, identity_B)\n",
    "            loss_G_B2A = loss_fn.generator_loss(discriminator_A(fake_A), real_B, reconstructed_B, identity_A)\n",
    "            loss_G = loss_G_A2B + loss_G_B2A\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ------------------------\n",
    "            # Train Discriminators\n",
    "            # ------------------------\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            fake_A_detached = fake_A.detach()\n",
    "            fake_B_detached = fake_B.detach()\n",
    "\n",
    "            loss_D_A = loss_fn.discriminator_loss(discriminator_A(real_A), discriminator_A(fake_A_detached))\n",
    "            loss_D_B = loss_fn.discriminator_loss(discriminator_B(real_B), discriminator_B(fake_B_detached))\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Accumulate epoch losses\n",
    "            epoch_generator_loss += loss_G.item()\n",
    "            epoch_discriminator_loss += loss_D.item()\n",
    "\n",
    "        # Compute average losses for the epoch\n",
    "        avg_generator_loss = epoch_generator_loss / num_batches\n",
    "        avg_discriminator_loss = epoch_discriminator_loss / num_batches\n",
    "        generator_losses.append(avg_generator_loss)\n",
    "        discriminator_losses.append(avg_discriminator_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Generator Loss: {avg_generator_loss:.4f}, Discriminator Loss: {avg_discriminator_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after every epoch\n",
    "        checkpoint = {\n",
    "            \"generator_A2B\": generator_A2B.state_dict(),\n",
    "            \"generator_B2A\": generator_B2A.state_dict(),\n",
    "            \"discriminator_A\": discriminator_A.state_dict(),\n",
    "            \"discriminator_B\": discriminator_B.state_dict(),\n",
    "            \"optimizer_G\": optimizer_G.state_dict(),\n",
    "            \"optimizer_D\": optimizer_D.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"generator_losses\": generator_losses,\n",
    "            \"discriminator_losses\": discriminator_losses,\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, \"last_checkpoint.pth\"))\n",
    "\n",
    "        # Save models every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(generator_A2B.state_dict(), os.path.join(checkpoint_dir, f\"generator_A2B_epoch{epoch+1}.pth\"))\n",
    "            torch.save(generator_B2A.state_dict(), os.path.join(checkpoint_dir, f\"generator_B2A_epoch{epoch+1}.pth\"))\n",
    "            torch.save(discriminator_A.state_dict(), os.path.join(checkpoint_dir, f\"discriminator_A_epoch{epoch+1}.pth\"))\n",
    "            torch.save(discriminator_B.state_dict(), os.path.join(checkpoint_dir, f\"discriminator_B_epoch{epoch+1}.pth\"))\n",
    "\n",
    "    return generator_losses, discriminator_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RZy00xNKwCL"
   },
   "source": [
    "### **Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHmjpxJXKz4A"
   },
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def plot_losses(generator_losses, discriminator_losses):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Generator Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(generator_losses) + 1), generator_losses, label=\"Generator Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Generator Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Discriminator Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(discriminator_losses) + 1), discriminator_losses, label=\"Discriminator Loss\", color=\"orange\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Discriminator Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn6PS7QJTFyY"
   },
   "source": [
    "### **Run the Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c7i_T5SKLVj"
   },
   "outputs": [],
   "source": [
    "def train_cycle_gan_model(\n",
    "    data_A_dir,\n",
    "    data_B_dir,\n",
    "    batch_size=16,\n",
    "    epochs=40,\n",
    "    generator_lr=0.0002,\n",
    "    discriminator_lr=0.0001,\n",
    "    lambda_cycle=10.0,\n",
    "    lambda_identity=5.0,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a CycleGAN model with the given parameters.\n",
    "\n",
    "    Args:\n",
    "        data_A_dir (str): Path to dataset A.\n",
    "        data_B_dir (str): Path to dataset B.\n",
    "        batch_size (int): Batch size for training.\n",
    "        epochs (int): Number of training epochs.\n",
    "        generator_lr (float): Learning rate for the generators.\n",
    "        discriminator_lr (float): Learning rate for the discriminators.\n",
    "        lambda_cycle (float): Weight for cycle consistency loss.\n",
    "        lambda_identity (float): Weight for identity loss.\n",
    "        checkpoint_dir (str): Directory to save model checkpoints.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    dataloader_A = load_data(data_A_dir, batch_size=batch_size)\n",
    "    dataloader_B = load_data(data_B_dir, batch_size=batch_size)\n",
    "\n",
    "    # Initialize models\n",
    "    generator_A2B = Generator(3, 3).to(device)\n",
    "    generator_B2A = Generator(3, 3).to(device)\n",
    "    discriminator_A = Discriminator(3).to(device)\n",
    "    discriminator_B = Discriminator(3).to(device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    loss_fn = CycleGANLosses(lambda_cycle=lambda_cycle, lambda_identity=lambda_identity)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(itertools.chain(generator_A2B.parameters(), generator_B2A.parameters()),\n",
    "                             lr=generator_lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(itertools.chain(discriminator_A.parameters(), discriminator_B.parameters()),\n",
    "                             lr=discriminator_lr, betas=(0.5, 0.999))\n",
    "\n",
    "    # Train the CycleGAN model\n",
    "    generator_losses, discriminator_losses = train_cycle_gan(\n",
    "        generator_A2B, generator_B2A, discriminator_A, discriminator_B,\n",
    "        dataloader_A, dataloader_B, optimizer_G, optimizer_D,\n",
    "        loss_fn, epochs, device, checkpoint_dir\n",
    "    )\n",
    "\n",
    "    # Plot the losses\n",
    "    plot_losses(generator_losses, discriminator_losses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
